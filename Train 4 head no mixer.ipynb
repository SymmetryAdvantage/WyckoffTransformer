{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T08:03:54.183165Z",
     "iopub.status.busy": "2024-05-24T08:03:54.182551Z",
     "iopub.status.idle": "2024-05-24T08:03:54.191562Z",
     "shell.execute_reply": "2024-05-24T08:03:54.190984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=Train 4 head no mixer.ipynb\n",
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "env: PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_NOTEBOOK_NAME Train 4 head no mixer.ipynb\n",
    "%env CUDA_DEVICE_ORDER PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES 1\n",
    "%env PYTORCH_CUDA_ALLOC_CONF backend:cudaMallocAsync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T08:03:54.211619Z",
     "iopub.status.busy": "2024-05-24T08:03:54.210931Z",
     "iopub.status.idle": "2024-05-24T08:03:55.041584Z",
     "shell.execute_reply": "2024-05-24T08:03:55.040838Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T08:03:55.044106Z",
     "iopub.status.busy": "2024-05-24T08:03:55.043902Z",
     "iopub.status.idle": "2024-05-24T08:03:58.921071Z",
     "shell.execute_reply": "2024-05-24T08:03:58.920567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 7 8 9\n"
     ]
    }
   ],
   "source": [
    "from mp_20_utils import load_all_data\n",
    "device = 'cuda'\n",
    "dataset = 'mp_20_biternary'\n",
    "datasets_pd, torch_datasets, site_to_ids, element_to_ids, spacegroup_to_ids, max_len, max_enumeration, enumeration_stop, enumeration_pad = load_all_data(\n",
    "    dataset=dataset)\n",
    "print(max_len, max_enumeration, enumeration_stop, enumeration_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T08:03:58.922894Z",
     "iopub.status.busy": "2024-05-24T08:03:58.922644Z",
     "iopub.status.idle": "2024-05-24T08:03:59.291464Z",
     "shell.execute_reply": "2024-05-24T08:03:59.290812Z"
    }
   },
   "outputs": [],
   "source": [
    "from cascade_transformer.model import CascadeTransformer\n",
    "from wyckoff_transformer import WyckoffTrainer\n",
    "from tokenization import PAD_TOKEN, MASK_TOKEN\n",
    "n_space_groups = len(spacegroup_to_ids)\n",
    "# Not all 230 space groups are present in the data\n",
    "# Embedding doesn't support uint8. Sad!\n",
    "dtype = torch.int64\n",
    "cascade_order = (\"elements\", \"symmetry_sites\", \"symmetry_sites_enumeration\")\n",
    "# (N_i, d_i, pad_i)\n",
    "assert max_enumeration + 1 == enumeration_stop\n",
    "assert max_enumeration + 2 == enumeration_pad\n",
    "enumeration_mask = max_enumeration + 3\n",
    "assert enumeration_mask < torch.iinfo(dtype).max\n",
    "\n",
    "cascade = (\n",
    "    (len(element_to_ids), 64, torch.tensor(element_to_ids[PAD_TOKEN], dtype=dtype, device=device)),\n",
    "    (len(site_to_ids), 64 - 1, torch.tensor(site_to_ids[PAD_TOKEN], dtype=dtype, device=device)),\n",
    "    (enumeration_mask + 1, None, torch.tensor(enumeration_pad, dtype=dtype, device=device))\n",
    ")\n",
    "model = CascadeTransformer(\n",
    "    n_start=n_space_groups,\n",
    "    cascade=cascade,\n",
    "    n_head=4,\n",
    "    d_hid=256,\n",
    "    n_layers=4,\n",
    "    dropout=0.1,\n",
    "    use_mixer=False).to(device)\n",
    "# Our dynamic discard of predicting PAD calls for frequent recompilation\n",
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T08:03:59.294580Z",
     "iopub.status.busy": "2024-05-24T08:03:59.294066Z",
     "iopub.status.idle": "2024-05-24T11:13:02.563299Z",
     "shell.execute_reply": "2024-05-24T11:13:02.561909Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkazeev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/kna/WyckoffTransformer/wandb/run-20240524_160400-u1gfkpji\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfrosty-river-238\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kazeev/WyckoffTransformer\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kazeev/WyckoffTransformer/runs/u1gfkpji\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 val_loss_epoch 86.28123474121094 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kna/WyckoffTransformer/pytorch/torch/optim/lr_scheduler.py:1300: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 val_loss_epoch 54.89169692993164 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140 val_loss_epoch 49.71947479248047 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 val_loss_epoch 45.63631057739258 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1140 val_loss_epoch 44.860931396484375 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1160 val_loss_epoch 43.69648361206055 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1220 val_loss_epoch 38.825927734375 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1460 val_loss_epoch 38.034454345703125 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1560 val_loss_epoch 38.033111572265625 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1780 val_loss_epoch 36.998779296875 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1880 val_loss_epoch 36.805824279785156 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2200 val_loss_epoch 35.67499542236328 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2580 val_loss_epoch 35.660030364990234 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2720 val_loss_epoch 34.49345397949219 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4060 val_loss_epoch 34.33827590942383 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4080 val_loss_epoch 34.13359451293945 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4480 val_loss_epoch 33.72593307495117 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5460 val_loss_epoch 33.699668884277344 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5520 val_loss_epoch 33.052310943603516 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5560 val_loss_epoch 32.82109451293945 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6440 val_loss_epoch 32.78449630737305 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6960 val_loss_epoch 32.652732849121094 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7420 val_loss_epoch 32.47675323486328 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7440 val_loss_epoch 32.1143913269043 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7560 val_loss_epoch 32.059532165527344 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7840 val_loss_epoch 31.922780990600586 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8680 val_loss_epoch 31.88934898376465 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10360 val_loss_epoch 31.872167587280273 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10380 val_loss_epoch 31.811370849609375 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10420 val_loss_epoch 31.722091674804688 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10540 val_loss_epoch 31.645763397216797 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10560 val_loss_epoch 31.60563850402832 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10600 val_loss_epoch 31.516510009765625 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12760 val_loss_epoch 31.504451751708984 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12840 val_loss_epoch 31.49676513671875 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12860 val_loss_epoch 31.455251693725586 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12980 val_loss_epoch 31.439817428588867 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13080 val_loss_epoch 31.398475646972656 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13100 val_loss_epoch 31.374202728271484 saved to checkpoints/2024-05-24_16-03-59/best_model_params.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 2.771 MB of 2.771 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 2.771 MB of 2.779 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.483 MB of 2.779 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 2.779 MB of 2.779 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 2.779 MB of 2.779 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 2.779 MB of 2.779 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: known_cascade_len ██▁▅▅▅█▁▅▅█▅▁▁▅█▁█▁▅▁▅▅▁█▅▅█▁▅█▅▁▁▁█▅█▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     known_seq_len █▁▆▄▁▄▆▆▅▃▇█▄▂█▅█▆█▅█▃▆▇▃▅▅▇▇▂▃▅▂▃▁█▆▅▁▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                lr █▆▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_batch ▁▄▁▁▆▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▄▁█▁▁▁█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_epoch █▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_loss_epoch █▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             epoch 40000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: known_cascade_len 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     known_seq_len 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_batch 37.45678\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_epoch 31.40739\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_loss_epoch 31.56444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfrosty-river-238\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/kazeev/WyckoffTransformer/runs/u1gfkpji\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/kazeev/WyckoffTransformer\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240524_160400-u1gfkpji/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "pad_dict = {\n",
    "    \"elements\": element_to_ids[PAD_TOKEN],\n",
    "    \"symmetry_sites\": site_to_ids[PAD_TOKEN],\n",
    "    \"symmetry_sites_enumeration\": enumeration_pad\n",
    "}\n",
    "mask_dict = {\n",
    "    \"elements\": element_to_ids[MASK_TOKEN],\n",
    "    \"symmetry_sites\": site_to_ids[MASK_TOKEN],\n",
    "    \"symmetry_sites_enumeration\": enumeration_mask\n",
    "}\n",
    "trainer = WyckoffTrainer(\n",
    "    model, torch_datasets, pad_dict, mask_dict, cascade_order, \"spacegroup_number\", max_len, device, dtype=dtype)\n",
    "trainer.train(epochs=40000, val_period=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wyckofftransformer-FeCwefly-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
