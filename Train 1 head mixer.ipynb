{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T08:03:51.849718Z",
     "iopub.status.busy": "2024-05-24T08:03:51.849407Z",
     "iopub.status.idle": "2024-05-24T08:03:51.856907Z",
     "shell.execute_reply": "2024-05-24T08:03:51.856226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=Train 1 head mixer.ipynb\n",
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_NOTEBOOK_NAME Train 1 head mixer.ipynb\n",
    "%env CUDA_DEVICE_ORDER PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES 0\n",
    "%env PYTORCH_CUDA_ALLOC_CONF backend:cudaMallocAsync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T08:03:51.891764Z",
     "iopub.status.busy": "2024-05-24T08:03:51.890166Z",
     "iopub.status.idle": "2024-05-24T08:03:52.677621Z",
     "shell.execute_reply": "2024-05-24T08:03:52.676874Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T08:03:52.682700Z",
     "iopub.status.busy": "2024-05-24T08:03:52.682546Z",
     "iopub.status.idle": "2024-05-24T08:03:56.656090Z",
     "shell.execute_reply": "2024-05-24T08:03:56.655549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 7 8 9\n"
     ]
    }
   ],
   "source": [
    "from mp_20_utils import load_all_data\n",
    "device = 'cuda'\n",
    "dataset = 'mp_20_biternary'\n",
    "datasets_pd, torch_datasets, site_to_ids, element_to_ids, spacegroup_to_ids, max_len, max_enumeration, enumeration_stop, enumeration_pad = load_all_data(\n",
    "    dataset=dataset)\n",
    "print(max_len, max_enumeration, enumeration_stop, enumeration_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T08:03:56.658666Z",
     "iopub.status.busy": "2024-05-24T08:03:56.658405Z",
     "iopub.status.idle": "2024-05-24T08:03:57.110095Z",
     "shell.execute_reply": "2024-05-24T08:03:57.109519Z"
    }
   },
   "outputs": [],
   "source": [
    "from cascade_transformer.model import CascadeTransformer\n",
    "from wyckoff_transformer import WyckoffTrainer\n",
    "from tokenization import PAD_TOKEN, MASK_TOKEN\n",
    "n_space_groups = len(spacegroup_to_ids)\n",
    "# Not all 230 space groups are present in the data\n",
    "# Embedding doesn't support uint8. Sad!\n",
    "dtype = torch.int64\n",
    "cascade_order = (\"elements\", \"symmetry_sites\", \"symmetry_sites_enumeration\")\n",
    "# (N_i, d_i, pad_i)\n",
    "assert max_enumeration + 1 == enumeration_stop\n",
    "assert max_enumeration + 2 == enumeration_pad\n",
    "enumeration_mask = max_enumeration + 3\n",
    "assert enumeration_mask < torch.iinfo(dtype).max\n",
    "\n",
    "cascade = (\n",
    "    (len(element_to_ids), 64, torch.tensor(element_to_ids[PAD_TOKEN], dtype=dtype, device=device)),\n",
    "    (len(site_to_ids), 64 - 1, torch.tensor(site_to_ids[PAD_TOKEN], dtype=dtype, device=device)),\n",
    "    (enumeration_mask + 1, None, torch.tensor(enumeration_pad, dtype=dtype, device=device))\n",
    ")\n",
    "model = CascadeTransformer(\n",
    "    n_start=n_space_groups,\n",
    "    cascade=cascade,\n",
    "    n_head=1,\n",
    "    d_hid=256,\n",
    "    n_layers=4,\n",
    "    dropout=0.1,\n",
    "    use_mixer=True).to(device)\n",
    "# Our dynamic discard of predicting PAD calls for frequent recompilation\n",
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T08:03:57.112251Z",
     "iopub.status.busy": "2024-05-24T08:03:57.112107Z",
     "iopub.status.idle": "2024-05-24T11:15:06.205486Z",
     "shell.execute_reply": "2024-05-24T11:15:06.204282Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkazeev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/kna/WyckoffTransformer/wandb/run-20240524_160358-nf7qfrff\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdeft-plasma-237\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kazeev/WyckoffTransformer\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kazeev/WyckoffTransformer/runs/nf7qfrff\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 val_loss_epoch 90.71502685546875 saved to checkpoints/2024-05-24_16-03-57/best_model_params.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kna/WyckoffTransformer/pytorch/torch/optim/lr_scheduler.py:1300: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 val_loss_epoch 51.163509368896484 saved to checkpoints/2024-05-24_16-03-57/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240 val_loss_epoch 48.3653678894043 saved to checkpoints/2024-05-24_16-03-57/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 660 val_loss_epoch 47.044532775878906 saved to checkpoints/2024-05-24_16-03-57/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 840 val_loss_epoch 46.41947555541992 saved to checkpoints/2024-05-24_16-03-57/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2680 val_loss_epoch 45.86494445800781 saved to checkpoints/2024-05-24_16-03-57/best_model_params.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 2.834 MB of 2.834 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.495 MB of 2.839 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 2.777 MB of 2.839 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 2.777 MB of 2.839 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 2.839 MB of 2.839 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: known_cascade_len ████▁▁▅▁▅███▁▁▁▅█▁▁▅▅▅█▅▅▁█▅██▅█▅█▅▅▁▁██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     known_seq_len ▃▅▂▁▃▇▆▃▁▇▄▆▃█▂▁▁▂█▄▅▂▃▃▁▂▇▄▄▅▄▁▃▇█▅▃▆▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                lr ██▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_batch ▁▁▂▃▂▁▁▁█▁▁▁▁▁▃█▃▄▁▁▁▄▁▁█▄▁▁▁▁▁▃▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_epoch ▂█▂▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_loss_epoch ▂█▂▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             epoch 40000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: known_cascade_len 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     known_seq_len 17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_batch 142.2659\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_epoch 48.92275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_loss_epoch 48.82223\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdeft-plasma-237\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/kazeev/WyckoffTransformer/runs/nf7qfrff\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/kazeev/WyckoffTransformer\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240524_160358-nf7qfrff/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "pad_dict = {\n",
    "    \"elements\": element_to_ids[PAD_TOKEN],\n",
    "    \"symmetry_sites\": site_to_ids[PAD_TOKEN],\n",
    "    \"symmetry_sites_enumeration\": enumeration_pad\n",
    "}\n",
    "mask_dict = {\n",
    "    \"elements\": element_to_ids[MASK_TOKEN],\n",
    "    \"symmetry_sites\": site_to_ids[MASK_TOKEN],\n",
    "    \"symmetry_sites_enumeration\": enumeration_mask\n",
    "}\n",
    "trainer = WyckoffTrainer(\n",
    "    model, torch_datasets, pad_dict, mask_dict, cascade_order, \"spacegroup_number\", max_len, device, dtype=dtype)\n",
    "trainer.train(epochs=40000, val_period=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wyckofftransformer-FeCwefly-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
