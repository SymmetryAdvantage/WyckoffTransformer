{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T10:32:13.648609Z",
     "iopub.status.busy": "2024-05-23T10:32:13.648478Z",
     "iopub.status.idle": "2024-05-23T10:32:13.654708Z",
     "shell.execute_reply": "2024-05-23T10:32:13.654232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=Train.ipynb\n",
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_NOTEBOOK_NAME Train.ipynb\n",
    "%env CUDA_DEVICE_ORDER PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES 0\n",
    "%env PYTORCH_CUDA_ALLOC_CONF backend:cudaMallocAsync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T10:32:13.673181Z",
     "iopub.status.busy": "2024-05-23T10:32:13.672935Z",
     "iopub.status.idle": "2024-05-23T10:32:26.516198Z",
     "shell.execute_reply": "2024-05-23T10:32:26.515029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 7 8 9\n"
     ]
    }
   ],
   "source": [
    "from mp_20_utils import load_all_data\n",
    "device = 'cuda'\n",
    "dataset = 'mp_20_biternary'\n",
    "datasets_pd, torch_datasets, site_to_ids, element_to_ids, spacegroup_to_ids, max_len, max_enumeration, enumeration_stop, enumeration_pad = load_all_data(\n",
    "    dataset=dataset)\n",
    "print(max_len, max_enumeration, enumeration_stop, enumeration_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T10:32:26.596526Z",
     "iopub.status.busy": "2024-05-23T10:32:26.596355Z",
     "iopub.status.idle": "2024-05-23T10:32:26.871115Z",
     "shell.execute_reply": "2024-05-23T10:32:26.870558Z"
    }
   },
   "outputs": [],
   "source": [
    "from cascade_transformer.model import CascadeTransformer\n",
    "from wyckoff_transformer import WyckoffTrainer\n",
    "from tokenization import PAD_TOKEN, MASK_TOKEN\n",
    "n_space_groups = len(spacegroup_to_ids)\n",
    "# Not all 230 space groups are present in the data\n",
    "# Embedding doesn't support uint8. Sad!\n",
    "dtype = torch.int64\n",
    "cascade_order = (\"elements\", \"symmetry_sites\", \"symmetry_sites_enumeration\")\n",
    "# (N_i, d_i, pad_i)\n",
    "assert max_enumeration + 1 == enumeration_stop\n",
    "assert max_enumeration + 2 == enumeration_pad\n",
    "enumeration_mask = max_enumeration + 3\n",
    "assert enumeration_mask < torch.iinfo(dtype).max\n",
    "\n",
    "cascade = (\n",
    "    (len(element_to_ids), 64, torch.tensor(element_to_ids[PAD_TOKEN], dtype=dtype, device=device)),\n",
    "    (len(site_to_ids), 64 - 1, torch.tensor(site_to_ids[PAD_TOKEN], dtype=dtype, device=device)),\n",
    "    (enumeration_mask + 1, None, torch.tensor(enumeration_pad, dtype=dtype, device=device))\n",
    ")\n",
    "model = CascadeTransformer(\n",
    "    n_start=n_space_groups,\n",
    "    cascade=cascade,\n",
    "    n_head=4,\n",
    "    d_hid=256,\n",
    "    n_layers=4,\n",
    "    dropout=0.1,\n",
    "    use_mixer=True).to(device)\n",
    "# Our dynamic discard of predicting PAD calls for frequent recompilation\n",
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T10:32:26.873525Z",
     "iopub.status.busy": "2024-05-23T10:32:26.873396Z",
     "iopub.status.idle": "2024-05-23T11:43:47.860074Z",
     "shell.execute_reply": "2024-05-23T11:43:47.858662Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkazeev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kna/WyckoffTransformer/wandb/run-20240524_022335-pvi5irzr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kazeev/WyckoffTransformer/runs/pvi5irzr' target=\"_blank\">trim-feather-227</a></strong> to <a href='https://wandb.ai/kazeev/WyckoffTransformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kazeev/WyckoffTransformer' target=\"_blank\">https://wandb.ai/kazeev/WyckoffTransformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kazeev/WyckoffTransformer/runs/pvi5irzr' target=\"_blank\">https://wandb.ai/kazeev/WyckoffTransformer/runs/pvi5irzr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 val_loss_epoch 63.65313720703125 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kna/WyckoffTransformer/pytorch/torch/optim/lr_scheduler.py:1300: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120 val_loss_epoch 51.414215087890625 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 280 val_loss_epoch 47.08189392089844 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 320 val_loss_epoch 46.95974349975586 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 440 val_loss_epoch 46.286102294921875 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 620 val_loss_epoch 45.68657302856445 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 880 val_loss_epoch 45.53960037231445 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 920 val_loss_epoch 45.190494537353516 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 940 val_loss_epoch 43.77199172973633 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 1260 val_loss_epoch 43.431400299072266 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 1480 val_loss_epoch 42.27555465698242 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 1520 val_loss_epoch 41.970726013183594 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 1840 val_loss_epoch 40.63697052001953 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 2780 val_loss_epoch 40.3381233215332 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 2900 val_loss_epoch 40.29111862182617 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 3200 val_loss_epoch 40.038604736328125 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 3240 val_loss_epoch 39.97623062133789 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 3340 val_loss_epoch 39.72093200683594 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 3700 val_loss_epoch 39.56884002685547 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 3720 val_loss_epoch 38.86573028564453 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 3740 val_loss_epoch 38.3679084777832 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 3760 val_loss_epoch 38.28660583496094 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 3800 val_loss_epoch 37.809688568115234 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 4000 val_loss_epoch 37.40327072143555 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 4040 val_loss_epoch 36.8985710144043 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 4160 val_loss_epoch 36.64387512207031 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 4880 val_loss_epoch 36.366939544677734 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 5400 val_loss_epoch 36.21625900268555 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 5540 val_loss_epoch 36.01719665527344 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 5680 val_loss_epoch 35.91983413696289 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 5700 val_loss_epoch 35.65357208251953 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 6080 val_loss_epoch 35.638916015625 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 6120 val_loss_epoch 35.47712707519531 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 6140 val_loss_epoch 35.47359085083008 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 6200 val_loss_epoch 35.46834182739258 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 6300 val_loss_epoch 35.45446014404297 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 6360 val_loss_epoch 35.423728942871094 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 6680 val_loss_epoch 35.201087951660156 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 6800 val_loss_epoch 35.097164154052734 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 7020 val_loss_epoch 35.08382034301758 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 7040 val_loss_epoch 35.01907730102539 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n",
      "Epoch 7060 val_loss_epoch 34.88460159301758 saved to checkpoints/2024-05-24_02-23-34/best_model_params.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19af5cd13a234aecb73677091fe53335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.764 MB of 1.764 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>known_cascade_len</td><td>▁█▁▅▁▁▁▁█▅█▁▁████▅▅██▁▅█▅▅▁▅▁▅▁█▁▅█▁▁▅▅▅</td></tr><tr><td>known_seq_len</td><td>▂▄▁▃▂▂▇▁█▂▂▃▃█▇▇▂▆▄▄▆▁▆▆▁▆▃█▃▂▃▅█▃▄▅▂▃▄▃</td></tr><tr><td>lr</td><td>█▇▇▆▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_batch</td><td>▄▁█▁▂▂▁▆▁▃▁▁▂▁▁▁▁▁▁▁▁▆▁▁▄▁▁▁▂▁▁▁▁▁▁▁▃▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▃▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>█▃▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10000</td></tr><tr><td>known_cascade_len</td><td>2</td></tr><tr><td>known_seq_len</td><td>18</td></tr><tr><td>lr</td><td>0.00728</td></tr><tr><td>train_loss_batch</td><td>5.60211</td></tr><tr><td>train_loss_epoch</td><td>35.17665</td></tr><tr><td>val_loss_epoch</td><td>35.07162</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trim-feather-227</strong> at: <a href='https://wandb.ai/kazeev/WyckoffTransformer/runs/pvi5irzr' target=\"_blank\">https://wandb.ai/kazeev/WyckoffTransformer/runs/pvi5irzr</a><br/> View project at: <a href='https://wandb.ai/kazeev/WyckoffTransformer' target=\"_blank\">https://wandb.ai/kazeev/WyckoffTransformer</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240524_022335-pvi5irzr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "pad_dict = {\n",
    "    \"elements\": element_to_ids[PAD_TOKEN],\n",
    "    \"symmetry_sites\": site_to_ids[PAD_TOKEN],\n",
    "    \"symmetry_sites_enumeration\": enumeration_pad\n",
    "}\n",
    "mask_dict = {\n",
    "    \"elements\": element_to_ids[MASK_TOKEN],\n",
    "    \"symmetry_sites\": site_to_ids[MASK_TOKEN],\n",
    "    \"symmetry_sites_enumeration\": enumeration_mask\n",
    "}\n",
    "trainer = WyckoffTrainer(\n",
    "    model, torch_datasets, pad_dict, mask_dict, cascade_order, \"spacegroup_number\", max_len, device, dtype=dtype)\n",
    "trainer.train(epochs=10000, val_period=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wyckofftransformer-FeCwefly-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
